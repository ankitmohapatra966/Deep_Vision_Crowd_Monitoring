{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543eb0ee-8a7e-443a-b4ce-1f439813b36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Loaded pretrained model\n",
      "Starting Validation...\n",
      "Validation MAE: 0.21, RMSE: 0.41\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.io import loadmat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "TRAIN_IMG_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\images\"\n",
    "TRAIN_GT_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\ground-truth\"\n",
    "TEST_IMG_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\images\"\n",
    "TEST_GT_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\ground-truth\"\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "DOWNSAMPLE_FACTOR = 8\n",
    "OUTPUT_SIZE = IMG_HEIGHT // DOWNSAMPLE_FACTOR\n",
    "\n",
    "def gaussian_filter_density(gt):\n",
    "    density = np.zeros(gt.shape, dtype=np.float32)\n",
    "    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))\n",
    "    if len(pts) == 0:\n",
    "        return density\n",
    "    \n",
    "    sigma = 15\n",
    "    for i in range(len(pts)):\n",
    "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
    "        y, x = pts[i][1], pts[i][0]\n",
    "        if y < gt.shape[0] and x < gt.shape[1]:\n",
    "             pt2d[y, x] = 1.\n",
    "        density += cv2.GaussianBlur(pt2d, (0,0), sigma, borderType=cv2.BORDER_CONSTANT)\n",
    "    return density\n",
    "\n",
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt_dir, transform=None, img_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "        self.img_paths = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        filename = os.path.basename(img_path).replace(\".jpg\",\".mat\")\n",
    "        mat_path = os.path.join(self.gt_dir, \"GT_\" + filename) \n",
    "        \n",
    "        if not os.path.exists(mat_path):\n",
    "            raise FileNotFoundError(f\"Ground truth file not found: {mat_path}\")\n",
    "\n",
    "        try:\n",
    "            mat = h5py.File(mat_path, 'r')\n",
    "            gt = np.array(mat['image_info'][0][0][0][0][0])\n",
    "            mat.close()\n",
    "        except OSError:\n",
    "            mat = loadmat(mat_path)\n",
    "            gt = np.array(mat['image_info'][0, 0]['location'])\n",
    "\n",
    "        gt = gt.flatten()\n",
    "\n",
    "        if gt.size == 0:\n",
    "            gt = np.empty((0, 2), dtype=np.int32)\n",
    "        elif gt.size % 2 != 0:\n",
    "            gt = np.empty((0, 2), dtype=np.int32)\n",
    "        else:\n",
    "            gt = gt.reshape(-1, 2).astype(np.int32)\n",
    "\n",
    "        h, w = img.size[1], img.size[0]\n",
    "\n",
    "        k = np.zeros((h, w))\n",
    "        for i in range(len(gt)):\n",
    "            x, y = gt[i][0], gt[i][1]\n",
    "            if y < h and x < w:\n",
    "                k[y, x] = 1\n",
    "\n",
    "        density = gaussian_filter_density(k)\n",
    "\n",
    "        img = img.resize(self.img_size)\n",
    "        \n",
    "        scale_factor = (self.img_size[0] * self.img_size[1]) / (h * w)\n",
    "        density = cv2.resize(density, (self.img_size[1], self.img_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "        density = density * scale_factor\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "        return img, density\n",
    "\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSRNet, self).__init__()\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33]) \n",
    "        \n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def train_model():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = CrowdDataset(TRAIN_IMG_DIR, TRAIN_GT_DIR, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    NUM_EPOCHS = 2\n",
    "    print(f\"Starting Training for {NUM_EPOCHS} epochs...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for imgs, densities in train_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            \n",
    "            target_densities_downsampled = F.interpolate(\n",
    "                densities, \n",
    "                size=(OUTPUT_SIZE, OUTPUT_SIZE), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            \n",
    "            target_densities_downsampled = target_densities_downsampled * (DOWNSAMPLE_FACTOR ** 2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = criterion(outputs, target_densities_downsampled) \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"csrnet_partA.pth\")\n",
    "    print(\"Model saved to csrnet_partA.pth\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_dataset = CrowdDataset(TEST_IMG_DIR, TEST_GT_DIR, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    print(\"Starting Validation...\")\n",
    "    model.eval()\n",
    "    mae, rmse, n = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, densities in val_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            predicted_count = outputs.sum().item()\n",
    "            actual_count = densities.sum().item()\n",
    "            \n",
    "            mae += abs(predicted_count - actual_count)\n",
    "            rmse += (predicted_count - actual_count)**2\n",
    "            n += 1\n",
    "    mae /= n\n",
    "    rmse = np.sqrt(rmse/n)\n",
    "    print(f\"Validation MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "def visualize_density(model, img_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_orig = np.array(img)\n",
    "    img = img.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "    \n",
    "    img_t = transform(img).unsqueeze(0).to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_t)\n",
    "        \n",
    "    density_map = output.squeeze().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_orig)\n",
    "    plt.title(\"Original Image\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(density_map, cmap='jet')\n",
    "    plt.title(\"Predicted Density Map\")\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"Predicted count:\", output.sum().item())\n",
    "\n",
    "def alert_system(model, img_path, crowd_limit=50):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_resized = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_t = transform(img_resized).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_t)\n",
    "        \n",
    "    count = int(output.sum().item())\n",
    "    \n",
    "    if count > crowd_limit:\n",
    "        print(f\"🚨 ALERT: Overcrowded! Count: {count} (Limit: {crowd_limit})\")\n",
    "        return True, count\n",
    "    else:\n",
    "        print(f\"✅ Normal crowd level. Count: {count} (Limit: {crowd_limit})\")\n",
    "        return False, count\n",
    "\n",
    "def process_images_with_alerts(model, img_dir, crowd_limit=50):\n",
    "    image_files = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "    alert_count = 0\n",
    "    total_images = 0\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images with crowd limit: {crowd_limit}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        filename = os.path.basename(img_path)\n",
    "        is_alert, count = alert_system(model, img_path, crowd_limit)\n",
    "        \n",
    "        if is_alert:\n",
    "            alert_count += 1\n",
    "        \n",
    "        total_images += 1\n",
    "        print(f\"Image: {filename} | Count: {count} | {'ALERT' if is_alert else 'NORMAL'}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Summary: {alert_count}/{total_images} images triggered alerts\")\n",
    "    print(f\"Alert percentage: {(alert_count/total_images)*100:.1f}%\")\n",
    "\n",
    "def load_pretrained_model():\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    if os.path.exists(\"csrnet_partA.pth\"):\n",
    "        model.load_state_dict(torch.load(\"csrnet_partA.pth\", map_location=DEVICE))\n",
    "        print(\"Loaded pretrained model\")\n",
    "    else:\n",
    "        print(\"No pretrained model found, training new model...\")\n",
    "        model = train_model()\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = load_pretrained_model()\n",
    "    evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d363ffe-623c-4077-ac0a-11eeeef37ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Training new model...\n",
      "Training for 5 epochs...\n",
      "Epoch 1/5, Loss: 0.005751, LR: 1.00e-05\n",
      "Best model saved with loss: 0.005751\n",
      "Epoch 2/5, Loss: 0.000604, LR: 1.00e-05\n",
      "Best model saved with loss: 0.000604\n",
      "Epoch 3/5, Loss: 0.000232, LR: 1.00e-05\n",
      "Best model saved with loss: 0.000232\n",
      "Epoch 4/5, Loss: 0.000103, LR: 1.00e-05\n",
      "Best model saved with loss: 0.000103\n",
      "Epoch 5/5, Loss: 0.000075, LR: 5.00e-06\n",
      "Best model saved with loss: 0.000075\n",
      "Final model saved to csrnet_partA_final.pth\n",
      "Evaluating model...\n",
      "MAE: 1.78, RMSE: 2.21\n",
      "✅ Normal crowd level. Count: -409 (Limit: 30)\n",
      "IMG_1.jpg            | Count: -409 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -4 (Limit: 30)\n",
      "IMG_10.jpg           | Count:   -4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_100.jpg          | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -12 (Limit: 30)\n",
      "IMG_101.jpg          | Count:  -12 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_102.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -259 (Limit: 30)\n",
      "IMG_103.jpg          | Count: -259 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -5 (Limit: 30)\n",
      "IMG_104.jpg          | Count:   -5 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_105.jpg          | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -8 (Limit: 30)\n",
      "IMG_106.jpg          | Count:   -8 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -8 (Limit: 30)\n",
      "IMG_107.jpg          | Count:   -8 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -9 (Limit: 30)\n",
      "IMG_108.jpg          | Count:   -9 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_109.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_11.jpg           | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_110.jpg          | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -483 (Limit: 30)\n",
      "IMG_111.jpg          | Count: -483 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -413 (Limit: 30)\n",
      "IMG_112.jpg          | Count: -413 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -85 (Limit: 30)\n",
      "IMG_113.jpg          | Count:  -85 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1246 (Limit: 30)\n",
      "IMG_114.jpg          | Count: -1246 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 5 (Limit: 30)\n",
      "IMG_115.jpg          | Count:    5 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_116.jpg          | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_117.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -7 (Limit: 30)\n",
      "IMG_118.jpg          | Count:   -7 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_119.jpg          | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2080 (Limit: 30)\n",
      "IMG_12.jpg           | Count: -2080 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -46 (Limit: 30)\n",
      "IMG_120.jpg          | Count:  -46 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1042 (Limit: 30)\n",
      "IMG_121.jpg          | Count: -1042 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -284 (Limit: 30)\n",
      "IMG_122.jpg          | Count: -284 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_123.jpg          | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_124.jpg          | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_125.jpg          | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_126.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2133 (Limit: 30)\n",
      "IMG_127.jpg          | Count: -2133 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -50 (Limit: 30)\n",
      "IMG_128.jpg          | Count:  -50 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 4 (Limit: 30)\n",
      "IMG_129.jpg          | Count:    4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_13.jpg           | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -6 (Limit: 30)\n",
      "IMG_130.jpg          | Count:   -6 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_131.jpg          | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -11 (Limit: 30)\n",
      "IMG_132.jpg          | Count:  -11 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -63 (Limit: 30)\n",
      "IMG_133.jpg          | Count:  -63 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_134.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_135.jpg          | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -4 (Limit: 30)\n",
      "IMG_136.jpg          | Count:   -4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_137.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_138.jpg          | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_139.jpg          | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_14.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_140.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -573 (Limit: 30)\n",
      "IMG_141.jpg          | Count: -573 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_142.jpg          | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -17 (Limit: 30)\n",
      "IMG_143.jpg          | Count:  -17 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 4 (Limit: 30)\n",
      "IMG_144.jpg          | Count:    4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -12 (Limit: 30)\n",
      "IMG_145.jpg          | Count:  -12 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -490 (Limit: 30)\n",
      "IMG_146.jpg          | Count: -490 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1130 (Limit: 30)\n",
      "IMG_147.jpg          | Count: -1130 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_148.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -11 (Limit: 30)\n",
      "IMG_149.jpg          | Count:  -11 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -4 (Limit: 30)\n",
      "IMG_15.jpg           | Count:   -4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_150.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -15 (Limit: 30)\n",
      "IMG_151.jpg          | Count:  -15 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_152.jpg          | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_153.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 3 (Limit: 30)\n",
      "IMG_154.jpg          | Count:    3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_155.jpg          | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -33 (Limit: 30)\n",
      "IMG_156.jpg          | Count:  -33 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -4 (Limit: 30)\n",
      "IMG_157.jpg          | Count:   -4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -458 (Limit: 30)\n",
      "IMG_158.jpg          | Count: -458 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_159.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -93 (Limit: 30)\n",
      "IMG_16.jpg           | Count:  -93 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -224 (Limit: 30)\n",
      "IMG_160.jpg          | Count: -224 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -259 (Limit: 30)\n",
      "IMG_161.jpg          | Count: -259 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -11 (Limit: 30)\n",
      "IMG_162.jpg          | Count:  -11 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -94 (Limit: 30)\n",
      "IMG_163.jpg          | Count:  -94 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_164.jpg          | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -120 (Limit: 30)\n",
      "IMG_165.jpg          | Count: -120 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -6 (Limit: 30)\n",
      "IMG_166.jpg          | Count:   -6 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_167.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -214 (Limit: 30)\n",
      "IMG_168.jpg          | Count: -214 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_169.jpg          | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 5 (Limit: 30)\n",
      "IMG_17.jpg           | Count:    5 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_170.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_171.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_172.jpg          | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -407 (Limit: 30)\n",
      "IMG_173.jpg          | Count: -407 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1354 (Limit: 30)\n",
      "IMG_174.jpg          | Count: -1354 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -9 (Limit: 30)\n",
      "IMG_175.jpg          | Count:   -9 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1701 (Limit: 30)\n",
      "IMG_176.jpg          | Count: -1701 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -128 (Limit: 30)\n",
      "IMG_177.jpg          | Count: -128 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -198 (Limit: 30)\n",
      "IMG_178.jpg          | Count: -198 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -329 (Limit: 30)\n",
      "IMG_179.jpg          | Count: -329 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -568 (Limit: 30)\n",
      "IMG_18.jpg           | Count: -568 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1827 (Limit: 30)\n",
      "IMG_180.jpg          | Count: -1827 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_181.jpg          | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -111 (Limit: 30)\n",
      "IMG_182.jpg          | Count: -111 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1703 (Limit: 30)\n",
      "IMG_19.jpg           | Count: -1703 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_2.jpg            | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_20.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -106 (Limit: 30)\n",
      "IMG_21.jpg           | Count: -106 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -499 (Limit: 30)\n",
      "IMG_22.jpg           | Count: -499 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -5 (Limit: 30)\n",
      "IMG_23.jpg           | Count:   -5 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_24.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -39 (Limit: 30)\n",
      "IMG_25.jpg           | Count:  -39 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_26.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -17 (Limit: 30)\n",
      "IMG_27.jpg           | Count:  -17 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_28.jpg           | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -729 (Limit: 30)\n",
      "IMG_29.jpg           | Count: -729 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_3.jpg            | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_30.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -5 (Limit: 30)\n",
      "IMG_31.jpg           | Count:   -5 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -44 (Limit: 30)\n",
      "IMG_32.jpg           | Count:  -44 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -10 (Limit: 30)\n",
      "IMG_33.jpg           | Count:  -10 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_34.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -493 (Limit: 30)\n",
      "IMG_35.jpg           | Count: -493 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1469 (Limit: 30)\n",
      "IMG_36.jpg           | Count: -1469 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_37.jpg           | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_38.jpg           | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_39.jpg           | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_4.jpg            | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1436 (Limit: 30)\n",
      "IMG_40.jpg           | Count: -1436 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_41.jpg           | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -75 (Limit: 30)\n",
      "IMG_42.jpg           | Count:  -75 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -200 (Limit: 30)\n",
      "IMG_43.jpg           | Count: -200 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -12 (Limit: 30)\n",
      "IMG_44.jpg           | Count:  -12 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -400 (Limit: 30)\n",
      "IMG_45.jpg           | Count: -400 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -4 (Limit: 30)\n",
      "IMG_46.jpg           | Count:   -4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_47.jpg           | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_48.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -140 (Limit: 30)\n",
      "IMG_49.jpg           | Count: -140 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 2 (Limit: 30)\n",
      "IMG_5.jpg            | Count:    2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -17 (Limit: 30)\n",
      "IMG_50.jpg           | Count:  -17 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_51.jpg           | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_52.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -158 (Limit: 30)\n",
      "IMG_53.jpg           | Count: -158 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -40 (Limit: 30)\n",
      "IMG_54.jpg           | Count:  -40 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_55.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 2 (Limit: 30)\n",
      "IMG_56.jpg           | Count:    2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -9 (Limit: 30)\n",
      "IMG_57.jpg           | Count:   -9 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -5 (Limit: 30)\n",
      "IMG_58.jpg           | Count:   -5 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -46 (Limit: 30)\n",
      "IMG_59.jpg           | Count:  -46 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_6.jpg            | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1909 (Limit: 30)\n",
      "IMG_60.jpg           | Count: -1909 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -13 (Limit: 30)\n",
      "IMG_61.jpg           | Count:  -13 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -6 (Limit: 30)\n",
      "IMG_62.jpg           | Count:   -6 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -96 (Limit: 30)\n",
      "IMG_63.jpg           | Count:  -96 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_64.jpg           | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_65.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -6 (Limit: 30)\n",
      "IMG_66.jpg           | Count:   -6 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -132 (Limit: 30)\n",
      "IMG_67.jpg           | Count: -132 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -4 (Limit: 30)\n",
      "IMG_68.jpg           | Count:   -4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2 (Limit: 30)\n",
      "IMG_69.jpg           | Count:   -2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -2734 (Limit: 30)\n",
      "IMG_7.jpg            | Count: -2734 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -29 (Limit: 30)\n",
      "IMG_70.jpg           | Count:  -29 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -7 (Limit: 30)\n",
      "IMG_71.jpg           | Count:   -7 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_72.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_73.jpg           | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_74.jpg           | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -6 (Limit: 30)\n",
      "IMG_75.jpg           | Count:   -6 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -16 (Limit: 30)\n",
      "IMG_76.jpg           | Count:  -16 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -218 (Limit: 30)\n",
      "IMG_77.jpg           | Count: -218 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -5 (Limit: 30)\n",
      "IMG_78.jpg           | Count:   -5 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_79.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1470 (Limit: 30)\n",
      "IMG_8.jpg            | Count: -1470 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -4 (Limit: 30)\n",
      "IMG_80.jpg           | Count:   -4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_81.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -270 (Limit: 30)\n",
      "IMG_82.jpg           | Count: -270 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_83.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_84.jpg           | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_85.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_86.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -4 (Limit: 30)\n",
      "IMG_87.jpg           | Count:   -4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -9 (Limit: 30)\n",
      "IMG_88.jpg           | Count:   -9 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_89.jpg           | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -52 (Limit: 30)\n",
      "IMG_9.jpg            | Count:  -52 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 4 (Limit: 30)\n",
      "IMG_90.jpg           | Count:    4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_91.jpg           | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -3 (Limit: 30)\n",
      "IMG_92.jpg           | Count:   -3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_93.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_94.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -4 (Limit: 30)\n",
      "IMG_95.jpg           | Count:   -4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_96.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_97.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -1 (Limit: 30)\n",
      "IMG_98.jpg           | Count:   -1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: -168 (Limit: 30)\n",
      "IMG_99.jpg           | Count: -168 | ✅ NORMAL\n",
      "Summary: 0/182 images triggered alerts\n",
      "Alert percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.io import loadmat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "TRAIN_IMG_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\images\"\n",
    "TRAIN_GT_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\ground-truth\"\n",
    "TEST_IMG_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\images\"\n",
    "TEST_GT_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\ground-truth\"\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "DOWNSAMPLE_FACTOR = 8\n",
    "OUTPUT_SIZE = IMG_HEIGHT // DOWNSAMPLE_FACTOR\n",
    "\n",
    "def gaussian_filter_density(gt):\n",
    "    density = np.zeros(gt.shape, dtype=np.float32)\n",
    "    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))\n",
    "    if len(pts) == 0:\n",
    "        return density\n",
    "    \n",
    "    sigma = 15\n",
    "    for i in range(len(pts)):\n",
    "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
    "        y, x = pts[i][1], pts[i][0]\n",
    "        if y < gt.shape[0] and x < gt.shape[1]:\n",
    "             pt2d[y, x] = 1.\n",
    "        density += cv2.GaussianBlur(pt2d, (0,0), sigma, borderType=cv2.BORDER_CONSTANT)\n",
    "    return density\n",
    "\n",
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt_dir, transform=None, img_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "        self.img_paths = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        filename = os.path.basename(img_path).replace(\".jpg\",\".mat\")\n",
    "        mat_path = os.path.join(self.gt_dir, \"GT_\" + filename) \n",
    "        \n",
    "        if not os.path.exists(mat_path):\n",
    "            raise FileNotFoundError(f\"Ground truth file not found: {mat_path}\")\n",
    "\n",
    "        try:\n",
    "            mat = h5py.File(mat_path, 'r')\n",
    "            gt = np.array(mat['image_info'][0][0][0][0][0])\n",
    "            mat.close()\n",
    "        except OSError:\n",
    "            mat = loadmat(mat_path)\n",
    "            gt = np.array(mat['image_info'][0, 0]['location'])\n",
    "\n",
    "        gt = gt.flatten()\n",
    "\n",
    "        if gt.size == 0:\n",
    "            gt = np.empty((0, 2), dtype=np.int32)\n",
    "        elif gt.size % 2 != 0:\n",
    "            gt = np.empty((0, 2), dtype=np.int32)\n",
    "        else:\n",
    "            gt = gt.reshape(-1, 2).astype(np.int32)\n",
    "\n",
    "        h, w = img.size[1], img.size[0]\n",
    "\n",
    "        k = np.zeros((h, w))\n",
    "        for i in range(len(gt)):\n",
    "            x, y = gt[i][0], gt[i][1]\n",
    "            if y < h and x < w:\n",
    "                k[y, x] = 1\n",
    "\n",
    "        density = gaussian_filter_density(k)\n",
    "\n",
    "        img = img.resize(self.img_size)\n",
    "        \n",
    "        scale_factor = (self.img_size[0] * self.img_size[1]) / (h * w)\n",
    "        density = cv2.resize(density, (self.img_size[1], self.img_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "        density = density * scale_factor\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "        return img, density\n",
    "\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSRNet, self).__init__()\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33]) \n",
    "        \n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "def train_model(num_epochs=5, batch_size=4, learning_rate=1e-5, save_every=5):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = CrowdDataset(TRAIN_IMG_DIR, TRAIN_GT_DIR, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for imgs, densities in train_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            \n",
    "            target_densities_downsampled = F.interpolate(\n",
    "                densities, \n",
    "                size=(OUTPUT_SIZE, OUTPUT_SIZE), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            \n",
    "            target_densities_downsampled = target_densities_downsampled * (DOWNSAMPLE_FACTOR ** 2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            loss = criterion(outputs, target_densities_downsampled) \n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), \"csrnet_partA_best.pth\")\n",
    "            print(f\"Best model saved with loss: {best_loss:.6f}\")\n",
    "        \n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save(model.state_dict(), f\"csrnet_partA_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"csrnet_partA_final.pth\")\n",
    "    print(\"Final model saved to csrnet_partA_final.pth\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_dataset = CrowdDataset(TEST_IMG_DIR, TEST_GT_DIR, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    mae, rmse, n = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, densities in val_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            predicted_count = outputs.sum().item()\n",
    "            actual_count = densities.sum().item()\n",
    "            \n",
    "            mae += abs(predicted_count - actual_count)\n",
    "            rmse += (predicted_count - actual_count)**2\n",
    "            n += 1\n",
    "    \n",
    "    mae /= n\n",
    "    rmse = np.sqrt(rmse/n)\n",
    "    \n",
    "    print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "def alert_system(model, img_path, crowd_limit=50):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_resized = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_t = transform(img_resized).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_t)\n",
    "        \n",
    "    count = int(output.sum().item())\n",
    "    \n",
    "    if count > crowd_limit:\n",
    "        print(f\"🚨 ALERT: Overcrowded! Count: {count} (Limit: {crowd_limit})\")\n",
    "        return True, count\n",
    "    else:\n",
    "        print(f\"✅ Normal crowd level. Count: {count} (Limit: {crowd_limit})\")\n",
    "        return False, count\n",
    "\n",
    "def process_images_with_alerts(model, img_dir, crowd_limit=50):\n",
    "    image_files = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "    alert_count = 0\n",
    "    total_images = len(image_files)\n",
    "    \n",
    "    if total_images == 0:\n",
    "        print(f\"No images found in {img_dir}\")\n",
    "        return\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        filename = os.path.basename(img_path)\n",
    "        is_alert, count = alert_system(model, img_path, crowd_limit)\n",
    "        status = \"🚨 ALERT\" if is_alert else \"✅ NORMAL\"\n",
    "        print(f\"{filename:<20} | Count: {count:>4} | {status}\")\n",
    "        if is_alert:\n",
    "            alert_count += 1\n",
    "    \n",
    "    print(f\"Summary: {alert_count}/{total_images} images triggered alerts\")\n",
    "    print(f\"Alert percentage: {(alert_count/total_images)*100:.1f}%\")\n",
    "\n",
    "def load_model(model_path=\"csrnet_partA_best.pth\"):\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "        return model, True\n",
    "    return model, False\n",
    "\n",
    "def main():\n",
    "    model, model_loaded = load_model()\n",
    "    \n",
    "    if not model_loaded:\n",
    "        print(\"Training new model...\")\n",
    "        model = train_model(num_epochs=5, batch_size=4, learning_rate=1e-5)\n",
    "    \n",
    "    evaluate_model(model)\n",
    "    \n",
    "    if os.path.exists(TEST_IMG_DIR):\n",
    "        process_images_with_alerts(model, TEST_IMG_DIR, crowd_limit=30)\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a9ca3e0-2d1c-4588-91dd-b31664212aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Loaded model from csrnet_partA_best.pth (strict=False)\n",
      "Evaluating model...\n",
      "MAE: 0.48, RMSE: 1.17\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_1.jpg            | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_10.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_100.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_101.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_102.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_103.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_104.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_105.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_106.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_107.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_108.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_109.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_11.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_110.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_111.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_112.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_113.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_114.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 5 (Limit: 30)\n",
      "IMG_115.jpg          | Count:    5 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_116.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_117.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_118.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_119.jpg          | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_12.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_120.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_121.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_122.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_123.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_124.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_125.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_126.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_127.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_128.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 4 (Limit: 30)\n",
      "IMG_129.jpg          | Count:    4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_13.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_130.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_131.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_132.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_133.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_134.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_135.jpg          | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_136.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_137.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_138.jpg          | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_139.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_14.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_140.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_141.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_142.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_143.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 4 (Limit: 30)\n",
      "IMG_144.jpg          | Count:    4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_145.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_146.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_147.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_148.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_149.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_15.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_150.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_151.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_152.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_153.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 3 (Limit: 30)\n",
      "IMG_154.jpg          | Count:    3 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_155.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_156.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_157.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_158.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_159.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_16.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_160.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_161.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_162.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_163.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_164.jpg          | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_165.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_166.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_167.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_168.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_169.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 5 (Limit: 30)\n",
      "IMG_17.jpg           | Count:    5 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_170.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_171.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_172.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_173.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_174.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_175.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_176.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_177.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_178.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_179.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_18.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_180.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_181.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_182.jpg          | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_19.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_2.jpg            | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_20.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_21.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_22.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_23.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_24.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_25.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_26.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_27.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_28.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_29.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_3.jpg            | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_30.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_31.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_32.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_33.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_34.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_35.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_36.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_37.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_38.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_39.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_4.jpg            | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_40.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_41.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_42.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_43.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_44.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_45.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_46.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_47.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_48.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_49.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 2 (Limit: 30)\n",
      "IMG_5.jpg            | Count:    2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_50.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_51.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_52.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_53.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_54.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_55.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 2 (Limit: 30)\n",
      "IMG_56.jpg           | Count:    2 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_57.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_58.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_59.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_6.jpg            | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_60.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_61.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_62.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_63.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_64.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_65.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_66.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_67.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_68.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_69.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_7.jpg            | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_70.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_71.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_72.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_73.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_74.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_75.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_76.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_77.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_78.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_79.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_8.jpg            | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_80.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_81.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_82.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_83.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_84.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_85.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_86.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_87.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_88.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_89.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_9.jpg            | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 4 (Limit: 30)\n",
      "IMG_90.jpg           | Count:    4 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_91.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_92.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 1 (Limit: 30)\n",
      "IMG_93.jpg           | Count:    1 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_94.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_95.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_96.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_97.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_98.jpg           | Count:    0 | ✅ NORMAL\n",
      "✅ Normal crowd level. Count: 0 (Limit: 30)\n",
      "IMG_99.jpg           | Count:    0 | ✅ NORMAL\n",
      "Summary: 0/182 images triggered alerts\n",
      "Alert percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.io import loadmat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "TRAIN_IMG_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\images\"\n",
    "TRAIN_GT_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\ground-truth\"\n",
    "TEST_IMG_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\images\"\n",
    "TEST_GT_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\ground-truth\"\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "DOWNSAMPLE_FACTOR = 8\n",
    "OUTPUT_SIZE = IMG_HEIGHT // DOWNSAMPLE_FACTOR\n",
    "\n",
    "# -------------------------------\n",
    "# Gaussian Density Map\n",
    "# -------------------------------\n",
    "def gaussian_filter_density(gt):\n",
    "    density = np.zeros(gt.shape, dtype=np.float32)\n",
    "    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))\n",
    "    if len(pts) == 0:\n",
    "        return density\n",
    "    \n",
    "    sigma = 15\n",
    "    for i in range(len(pts)):\n",
    "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
    "        y, x = pts[i][1], pts[i][0]\n",
    "        if y < gt.shape[0] and x < gt.shape[1]:\n",
    "            pt2d[y, x] = 1.\n",
    "        density += cv2.GaussianBlur(pt2d, (0,0), sigma, borderType=cv2.BORDER_CONSTANT)\n",
    "    return density\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset\n",
    "# -------------------------------\n",
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt_dir, transform=None, img_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "        self.img_paths = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        filename = os.path.basename(img_path).replace(\".jpg\",\".mat\")\n",
    "        mat_path = os.path.join(self.gt_dir, \"GT_\" + filename) \n",
    "        \n",
    "        if not os.path.exists(mat_path):\n",
    "            raise FileNotFoundError(f\"Ground truth file not found: {mat_path}\")\n",
    "\n",
    "        try:\n",
    "            mat = h5py.File(mat_path, 'r')\n",
    "            gt = np.array(mat['image_info'][0][0][0][0][0])\n",
    "            mat.close()\n",
    "        except OSError:\n",
    "            mat = loadmat(mat_path)\n",
    "            gt = np.array(mat['image_info'][0, 0]['location'])\n",
    "\n",
    "        gt = gt.flatten()\n",
    "        if gt.size == 0:\n",
    "            gt = np.empty((0, 2), dtype=np.int32)\n",
    "        elif gt.size % 2 != 0:\n",
    "            gt = np.empty((0, 2), dtype=np.int32)\n",
    "        else:\n",
    "            gt = gt.reshape(-1, 2).astype(np.int32)\n",
    "\n",
    "        h, w = img.size[1], img.size[0]\n",
    "\n",
    "        k = np.zeros((h, w))\n",
    "        for i in range(len(gt)):\n",
    "            x, y = gt[i][0], gt[i][1]\n",
    "            if y < h and x < w:\n",
    "                k[y, x] = 1\n",
    "\n",
    "        density = gaussian_filter_density(k)\n",
    "        img = img.resize(self.img_size)\n",
    "        \n",
    "        scale_factor = (self.img_size[0] * self.img_size[1]) / (h * w)\n",
    "        density = cv2.resize(density, (self.img_size[1], self.img_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "        density = density * scale_factor\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "        return img, density\n",
    "\n",
    "# -------------------------------\n",
    "# CSRNet Model\n",
    "# -------------------------------\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSRNet, self).__init__()\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33]) \n",
    "        \n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Conv2d(64, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "def train_model(num_epochs=5, batch_size=4, learning_rate=1e-5, save_every=5):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = CrowdDataset(TRAIN_IMG_DIR, TRAIN_GT_DIR, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for imgs, densities in train_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            \n",
    "            target_densities_downsampled = F.interpolate(\n",
    "                densities, \n",
    "                size=(OUTPUT_SIZE, OUTPUT_SIZE), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            target_densities_downsampled = target_densities_downsampled * (DOWNSAMPLE_FACTOR ** 2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, target_densities_downsampled) \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), \"csrnet_partA_best.pth\")\n",
    "            print(f\"Best model saved with loss: {best_loss:.6f}\")\n",
    "        \n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save(model.state_dict(), f\"csrnet_partA_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"csrnet_partA_final.pth\")\n",
    "    print(\"Final model saved to csrnet_partA_final.pth\")\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------\n",
    "def evaluate_model(model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    val_dataset = CrowdDataset(TEST_IMG_DIR, TEST_GT_DIR, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    mae, rmse, n = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, densities in val_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            predicted_count = max(outputs.sum().item(), 0)  # ensure non-negative\n",
    "            actual_count = densities.sum().item()\n",
    "            \n",
    "            mae += abs(predicted_count - actual_count)\n",
    "            rmse += (predicted_count - actual_count)**2\n",
    "            n += 1\n",
    "    \n",
    "    mae /= n\n",
    "    rmse = np.sqrt(rmse/n)\n",
    "    \n",
    "    print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "# -------------------------------\n",
    "# Alert System\n",
    "# -------------------------------\n",
    "def alert_system(model, img_path, crowd_limit=50):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_resized = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_t = transform(img_resized).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_t)\n",
    "        \n",
    "    count = max(int(output.sum().item()), 0)  # avoid negative counts\n",
    "    \n",
    "    if count > crowd_limit:\n",
    "        print(f\"🚨 ALERT: Overcrowded! Count: {count} (Limit: {crowd_limit})\")\n",
    "        return True, count\n",
    "    else:\n",
    "        print(f\"✅ Normal crowd level. Count: {count} (Limit: {crowd_limit})\")\n",
    "        return False, count\n",
    "\n",
    "def process_images_with_alerts(model, img_dir, crowd_limit=50):\n",
    "    image_files = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "    alert_count = 0\n",
    "    total_images = len(image_files)\n",
    "    \n",
    "    if total_images == 0:\n",
    "        print(f\"No images found in {img_dir}\")\n",
    "        return\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        filename = os.path.basename(img_path)\n",
    "        is_alert, count = alert_system(model, img_path, crowd_limit)\n",
    "        status = \"🚨 ALERT\" if is_alert else \"✅ NORMAL\"\n",
    "        print(f\"{filename:<20} | Count: {count:>4} | {status}\")\n",
    "        if is_alert:\n",
    "            alert_count += 1\n",
    "    \n",
    "    print(f\"Summary: {alert_count}/{total_images} images triggered alerts\")\n",
    "    print(f\"Alert percentage: {(alert_count/total_images)*100:.1f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model\n",
    "# -------------------------------\n",
    "def load_model(model_path=\"csrnet_partA_best.pth\"):\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    if os.path.exists(model_path):\n",
    "        state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "        model.load_state_dict(state_dict, strict=False)  # <--- FIX HERE\n",
    "        print(f\"Loaded model from {model_path} (strict=False)\")\n",
    "        return model, True\n",
    "    return model, False\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "def main():\n",
    "    model, model_loaded = load_model()\n",
    "    \n",
    "    if not model_loaded:\n",
    "        print(\"Training new model...\")\n",
    "        model = train_model(num_epochs=5, batch_size=4, learning_rate=1e-5)\n",
    "    \n",
    "    evaluate_model(model)\n",
    "    \n",
    "    if os.path.exists(TEST_IMG_DIR):\n",
    "        process_images_with_alerts(model, TEST_IMG_DIR, crowd_limit=30)\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78262826-cf68-4fd9-a307-33fd937aa8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Training new model...\n",
      "Training for 5 epochs...\n",
      "Epoch 1/5, Loss: 0.061510, LR: 1.00e-05\n",
      "Best model saved with loss: 0.061510\n",
      "Epoch 2/5, Loss: 0.053974, LR: 1.00e-05\n",
      "Best model saved with loss: 0.053974\n",
      "Epoch 3/5, Loss: 0.042215, LR: 1.00e-05\n",
      "Best model saved with loss: 0.042215\n",
      "Epoch 4/5, Loss: 0.040385, LR: 1.00e-05\n",
      "Best model saved with loss: 0.040385\n",
      "Epoch 5/5, Loss: 0.037369, LR: 5.00e-06\n",
      "Best model saved with loss: 0.037369\n",
      "Final model saved to csrnet_partA_final.pth\n",
      "Evaluating model...\n",
      "MAE: 118.39, RMSE: 133.38\n",
      "🚨 ALERT: Overcrowded! Count: 224 (Limit: 30)\n",
      "IMG_1.jpg            | Count:  224 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 240 (Limit: 30)\n",
      "IMG_10.jpg           | Count:  240 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 302 (Limit: 30)\n",
      "IMG_100.jpg          | Count:  302 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 205 (Limit: 30)\n",
      "IMG_101.jpg          | Count:  205 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 271 (Limit: 30)\n",
      "IMG_102.jpg          | Count:  271 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 201 (Limit: 30)\n",
      "IMG_103.jpg          | Count:  201 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 483 (Limit: 30)\n",
      "IMG_104.jpg          | Count:  483 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 224 (Limit: 30)\n",
      "IMG_105.jpg          | Count:  224 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 473 (Limit: 30)\n",
      "IMG_106.jpg          | Count:  473 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 174 (Limit: 30)\n",
      "IMG_107.jpg          | Count:  174 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 227 (Limit: 30)\n",
      "IMG_108.jpg          | Count:  227 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 225 (Limit: 30)\n",
      "IMG_109.jpg          | Count:  225 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 321 (Limit: 30)\n",
      "IMG_11.jpg           | Count:  321 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 257 (Limit: 30)\n",
      "IMG_110.jpg          | Count:  257 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 225 (Limit: 30)\n",
      "IMG_111.jpg          | Count:  225 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 286 (Limit: 30)\n",
      "IMG_112.jpg          | Count:  286 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 233 (Limit: 30)\n",
      "IMG_113.jpg          | Count:  233 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 174 (Limit: 30)\n",
      "IMG_114.jpg          | Count:  174 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 373 (Limit: 30)\n",
      "IMG_115.jpg          | Count:  373 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 323 (Limit: 30)\n",
      "IMG_116.jpg          | Count:  323 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 522 (Limit: 30)\n",
      "IMG_117.jpg          | Count:  522 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 216 (Limit: 30)\n",
      "IMG_118.jpg          | Count:  216 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 219 (Limit: 30)\n",
      "IMG_119.jpg          | Count:  219 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 421 (Limit: 30)\n",
      "IMG_12.jpg           | Count:  421 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 176 (Limit: 30)\n",
      "IMG_120.jpg          | Count:  176 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 447 (Limit: 30)\n",
      "IMG_121.jpg          | Count:  447 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 224 (Limit: 30)\n",
      "IMG_122.jpg          | Count:  224 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 196 (Limit: 30)\n",
      "IMG_123.jpg          | Count:  196 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 199 (Limit: 30)\n",
      "IMG_124.jpg          | Count:  199 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 271 (Limit: 30)\n",
      "IMG_125.jpg          | Count:  271 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 211 (Limit: 30)\n",
      "IMG_126.jpg          | Count:  211 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 489 (Limit: 30)\n",
      "IMG_127.jpg          | Count:  489 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 253 (Limit: 30)\n",
      "IMG_128.jpg          | Count:  253 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 163 (Limit: 30)\n",
      "IMG_129.jpg          | Count:  163 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 221 (Limit: 30)\n",
      "IMG_13.jpg           | Count:  221 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 308 (Limit: 30)\n",
      "IMG_130.jpg          | Count:  308 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 240 (Limit: 30)\n",
      "IMG_131.jpg          | Count:  240 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 255 (Limit: 30)\n",
      "IMG_132.jpg          | Count:  255 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 208 (Limit: 30)\n",
      "IMG_133.jpg          | Count:  208 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 151 (Limit: 30)\n",
      "IMG_134.jpg          | Count:  151 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 154 (Limit: 30)\n",
      "IMG_135.jpg          | Count:  154 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 163 (Limit: 30)\n",
      "IMG_136.jpg          | Count:  163 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 349 (Limit: 30)\n",
      "IMG_137.jpg          | Count:  349 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 220 (Limit: 30)\n",
      "IMG_138.jpg          | Count:  220 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 189 (Limit: 30)\n",
      "IMG_139.jpg          | Count:  189 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 259 (Limit: 30)\n",
      "IMG_14.jpg           | Count:  259 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 159 (Limit: 30)\n",
      "IMG_140.jpg          | Count:  159 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 238 (Limit: 30)\n",
      "IMG_141.jpg          | Count:  238 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 200 (Limit: 30)\n",
      "IMG_142.jpg          | Count:  200 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 297 (Limit: 30)\n",
      "IMG_143.jpg          | Count:  297 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 271 (Limit: 30)\n",
      "IMG_144.jpg          | Count:  271 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 290 (Limit: 30)\n",
      "IMG_145.jpg          | Count:  290 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 239 (Limit: 30)\n",
      "IMG_146.jpg          | Count:  239 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 393 (Limit: 30)\n",
      "IMG_147.jpg          | Count:  393 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 165 (Limit: 30)\n",
      "IMG_148.jpg          | Count:  165 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 241 (Limit: 30)\n",
      "IMG_149.jpg          | Count:  241 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 251 (Limit: 30)\n",
      "IMG_15.jpg           | Count:  251 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 483 (Limit: 30)\n",
      "IMG_150.jpg          | Count:  483 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 242 (Limit: 30)\n",
      "IMG_151.jpg          | Count:  242 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 244 (Limit: 30)\n",
      "IMG_152.jpg          | Count:  244 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 214 (Limit: 30)\n",
      "IMG_153.jpg          | Count:  214 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 301 (Limit: 30)\n",
      "IMG_154.jpg          | Count:  301 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 199 (Limit: 30)\n",
      "IMG_155.jpg          | Count:  199 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 183 (Limit: 30)\n",
      "IMG_156.jpg          | Count:  183 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 233 (Limit: 30)\n",
      "IMG_157.jpg          | Count:  233 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 308 (Limit: 30)\n",
      "IMG_158.jpg          | Count:  308 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 252 (Limit: 30)\n",
      "IMG_159.jpg          | Count:  252 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 320 (Limit: 30)\n",
      "IMG_16.jpg           | Count:  320 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 159 (Limit: 30)\n",
      "IMG_160.jpg          | Count:  159 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 170 (Limit: 30)\n",
      "IMG_161.jpg          | Count:  170 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 217 (Limit: 30)\n",
      "IMG_162.jpg          | Count:  217 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 227 (Limit: 30)\n",
      "IMG_163.jpg          | Count:  227 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 301 (Limit: 30)\n",
      "IMG_164.jpg          | Count:  301 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 201 (Limit: 30)\n",
      "IMG_165.jpg          | Count:  201 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 201 (Limit: 30)\n",
      "IMG_166.jpg          | Count:  201 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 189 (Limit: 30)\n",
      "IMG_167.jpg          | Count:  189 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 215 (Limit: 30)\n",
      "IMG_168.jpg          | Count:  215 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 255 (Limit: 30)\n",
      "IMG_169.jpg          | Count:  255 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 417 (Limit: 30)\n",
      "IMG_17.jpg           | Count:  417 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 284 (Limit: 30)\n",
      "IMG_170.jpg          | Count:  284 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 212 (Limit: 30)\n",
      "IMG_171.jpg          | Count:  212 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 379 (Limit: 30)\n",
      "IMG_172.jpg          | Count:  379 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 283 (Limit: 30)\n",
      "IMG_173.jpg          | Count:  283 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 196 (Limit: 30)\n",
      "IMG_174.jpg          | Count:  196 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 219 (Limit: 30)\n",
      "IMG_175.jpg          | Count:  219 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 384 (Limit: 30)\n",
      "IMG_176.jpg          | Count:  384 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 186 (Limit: 30)\n",
      "IMG_177.jpg          | Count:  186 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 307 (Limit: 30)\n",
      "IMG_178.jpg          | Count:  307 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 173 (Limit: 30)\n",
      "IMG_179.jpg          | Count:  173 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 238 (Limit: 30)\n",
      "IMG_18.jpg           | Count:  238 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 354 (Limit: 30)\n",
      "IMG_180.jpg          | Count:  354 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 212 (Limit: 30)\n",
      "IMG_181.jpg          | Count:  212 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 173 (Limit: 30)\n",
      "IMG_182.jpg          | Count:  173 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 278 (Limit: 30)\n",
      "IMG_19.jpg           | Count:  278 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 401 (Limit: 30)\n",
      "IMG_2.jpg            | Count:  401 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 320 (Limit: 30)\n",
      "IMG_20.jpg           | Count:  320 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 216 (Limit: 30)\n",
      "IMG_21.jpg           | Count:  216 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 240 (Limit: 30)\n",
      "IMG_22.jpg           | Count:  240 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 206 (Limit: 30)\n",
      "IMG_23.jpg           | Count:  206 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 277 (Limit: 30)\n",
      "IMG_24.jpg           | Count:  277 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 169 (Limit: 30)\n",
      "IMG_25.jpg           | Count:  169 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 233 (Limit: 30)\n",
      "IMG_26.jpg           | Count:  233 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 374 (Limit: 30)\n",
      "IMG_27.jpg           | Count:  374 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 192 (Limit: 30)\n",
      "IMG_28.jpg           | Count:  192 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 269 (Limit: 30)\n",
      "IMG_29.jpg           | Count:  269 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 226 (Limit: 30)\n",
      "IMG_3.jpg            | Count:  226 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 264 (Limit: 30)\n",
      "IMG_30.jpg           | Count:  264 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 240 (Limit: 30)\n",
      "IMG_31.jpg           | Count:  240 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 249 (Limit: 30)\n",
      "IMG_32.jpg           | Count:  249 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 176 (Limit: 30)\n",
      "IMG_33.jpg           | Count:  176 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 180 (Limit: 30)\n",
      "IMG_34.jpg           | Count:  180 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 183 (Limit: 30)\n",
      "IMG_35.jpg           | Count:  183 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 463 (Limit: 30)\n",
      "IMG_36.jpg           | Count:  463 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 177 (Limit: 30)\n",
      "IMG_37.jpg           | Count:  177 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 231 (Limit: 30)\n",
      "IMG_38.jpg           | Count:  231 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 177 (Limit: 30)\n",
      "IMG_39.jpg           | Count:  177 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 202 (Limit: 30)\n",
      "IMG_4.jpg            | Count:  202 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 374 (Limit: 30)\n",
      "IMG_40.jpg           | Count:  374 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 165 (Limit: 30)\n",
      "IMG_41.jpg           | Count:  165 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 277 (Limit: 30)\n",
      "IMG_42.jpg           | Count:  277 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 174 (Limit: 30)\n",
      "IMG_43.jpg           | Count:  174 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 208 (Limit: 30)\n",
      "IMG_44.jpg           | Count:  208 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 195 (Limit: 30)\n",
      "IMG_45.jpg           | Count:  195 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 189 (Limit: 30)\n",
      "IMG_46.jpg           | Count:  189 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 321 (Limit: 30)\n",
      "IMG_47.jpg           | Count:  321 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 262 (Limit: 30)\n",
      "IMG_48.jpg           | Count:  262 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 241 (Limit: 30)\n",
      "IMG_49.jpg           | Count:  241 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 370 (Limit: 30)\n",
      "IMG_5.jpg            | Count:  370 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 331 (Limit: 30)\n",
      "IMG_50.jpg           | Count:  331 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 269 (Limit: 30)\n",
      "IMG_51.jpg           | Count:  269 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 173 (Limit: 30)\n",
      "IMG_52.jpg           | Count:  173 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 207 (Limit: 30)\n",
      "IMG_53.jpg           | Count:  207 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 293 (Limit: 30)\n",
      "IMG_54.jpg           | Count:  293 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 157 (Limit: 30)\n",
      "IMG_55.jpg           | Count:  157 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 203 (Limit: 30)\n",
      "IMG_56.jpg           | Count:  203 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 242 (Limit: 30)\n",
      "IMG_57.jpg           | Count:  242 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 189 (Limit: 30)\n",
      "IMG_58.jpg           | Count:  189 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 324 (Limit: 30)\n",
      "IMG_59.jpg           | Count:  324 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 305 (Limit: 30)\n",
      "IMG_6.jpg            | Count:  305 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 540 (Limit: 30)\n",
      "IMG_60.jpg           | Count:  540 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 213 (Limit: 30)\n",
      "IMG_61.jpg           | Count:  213 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 254 (Limit: 30)\n",
      "IMG_62.jpg           | Count:  254 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 283 (Limit: 30)\n",
      "IMG_63.jpg           | Count:  283 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 215 (Limit: 30)\n",
      "IMG_64.jpg           | Count:  215 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 200 (Limit: 30)\n",
      "IMG_65.jpg           | Count:  200 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 295 (Limit: 30)\n",
      "IMG_66.jpg           | Count:  295 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 201 (Limit: 30)\n",
      "IMG_67.jpg           | Count:  201 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 184 (Limit: 30)\n",
      "IMG_68.jpg           | Count:  184 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 202 (Limit: 30)\n",
      "IMG_69.jpg           | Count:  202 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 465 (Limit: 30)\n",
      "IMG_7.jpg            | Count:  465 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 259 (Limit: 30)\n",
      "IMG_70.jpg           | Count:  259 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 235 (Limit: 30)\n",
      "IMG_71.jpg           | Count:  235 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 320 (Limit: 30)\n",
      "IMG_72.jpg           | Count:  320 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 173 (Limit: 30)\n",
      "IMG_73.jpg           | Count:  173 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 466 (Limit: 30)\n",
      "IMG_74.jpg           | Count:  466 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 233 (Limit: 30)\n",
      "IMG_75.jpg           | Count:  233 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 405 (Limit: 30)\n",
      "IMG_76.jpg           | Count:  405 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 176 (Limit: 30)\n",
      "IMG_77.jpg           | Count:  176 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 215 (Limit: 30)\n",
      "IMG_78.jpg           | Count:  215 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 192 (Limit: 30)\n",
      "IMG_79.jpg           | Count:  192 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 362 (Limit: 30)\n",
      "IMG_8.jpg            | Count:  362 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 170 (Limit: 30)\n",
      "IMG_80.jpg           | Count:  170 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 361 (Limit: 30)\n",
      "IMG_81.jpg           | Count:  361 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 179 (Limit: 30)\n",
      "IMG_82.jpg           | Count:  179 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 213 (Limit: 30)\n",
      "IMG_83.jpg           | Count:  213 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 186 (Limit: 30)\n",
      "IMG_84.jpg           | Count:  186 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 177 (Limit: 30)\n",
      "IMG_85.jpg           | Count:  177 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 243 (Limit: 30)\n",
      "IMG_86.jpg           | Count:  243 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 193 (Limit: 30)\n",
      "IMG_87.jpg           | Count:  193 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 158 (Limit: 30)\n",
      "IMG_88.jpg           | Count:  158 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 272 (Limit: 30)\n",
      "IMG_89.jpg           | Count:  272 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 199 (Limit: 30)\n",
      "IMG_9.jpg            | Count:  199 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 501 (Limit: 30)\n",
      "IMG_90.jpg           | Count:  501 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 163 (Limit: 30)\n",
      "IMG_91.jpg           | Count:  163 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 424 (Limit: 30)\n",
      "IMG_92.jpg           | Count:  424 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 212 (Limit: 30)\n",
      "IMG_93.jpg           | Count:  212 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 156 (Limit: 30)\n",
      "IMG_94.jpg           | Count:  156 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 192 (Limit: 30)\n",
      "IMG_95.jpg           | Count:  192 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 261 (Limit: 30)\n",
      "IMG_96.jpg           | Count:  261 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 368 (Limit: 30)\n",
      "IMG_97.jpg           | Count:  368 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 175 (Limit: 30)\n",
      "IMG_98.jpg           | Count:  175 | 🚨 ALERT\n",
      "🚨 ALERT: Overcrowded! Count: 163 (Limit: 30)\n",
      "IMG_99.jpg           | Count:  163 | 🚨 ALERT\n",
      "Summary: 182/182 images triggered alerts\n",
      "Alert percentage: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from scipy.io import loadmat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------------------\n",
    "# Config\n",
    "# -------------------------------\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "TRAIN_IMG_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\images\"\n",
    "TRAIN_GT_DIR = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\train_data\\ground-truth\"\n",
    "TEST_IMG_DIR  = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\images\"\n",
    "TEST_GT_DIR   = r\"D:\\Task For Infosys Internship\\archive\\ShanghaiTech\\part_A\\test_data\\ground-truth\"\n",
    "\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "DOWNSAMPLE_FACTOR = 8\n",
    "OUTPUT_SIZE = IMG_HEIGHT // DOWNSAMPLE_FACTOR\n",
    "\n",
    "# -------------------------------\n",
    "# Gaussian Density Map Generator\n",
    "# -------------------------------\n",
    "def gaussian_filter_density(gt):\n",
    "    density = np.zeros(gt.shape, dtype=np.float32)\n",
    "    pts = np.array(list(zip(np.nonzero(gt)[1], np.nonzero(gt)[0])))\n",
    "    if len(pts) == 0:\n",
    "        return density\n",
    "    sigma = 15\n",
    "    for i in range(len(pts)):\n",
    "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
    "        y, x = pts[i][1], pts[i][0]\n",
    "        if y < gt.shape[0] and x < gt.shape[1]:\n",
    "            pt2d[y, x] = 1.\n",
    "        density += cv2.GaussianBlur(pt2d, (0,0), sigma, borderType=cv2.BORDER_CONSTANT)\n",
    "    return density\n",
    "\n",
    "# -------------------------------\n",
    "# Dataset\n",
    "# -------------------------------\n",
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, img_dir, gt_dir, transform=None, img_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "        self.img_paths = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "        self.gt_dir = gt_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        filename_mat = os.path.basename(img_path).replace(\".jpg\", \".mat\")\n",
    "        filename_h5  = os.path.basename(img_path).replace(\".jpg\", \".h5\")\n",
    "        mat_path     = os.path.join(self.gt_dir, \"GT_\" + filename_mat)\n",
    "        h5_path      = os.path.join(self.gt_dir, \"GT_\" + filename_h5)\n",
    "        \n",
    "        # Try both .mat (original annotation) and .h5 (density map) file naming\n",
    "        if os.path.exists(h5_path):\n",
    "            with h5py.File(h5_path, 'r') as hf:\n",
    "                # Typical structure: 'density'\n",
    "                density = np.asarray(hf['density'])\n",
    "                img = img.resize(self.img_size)\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                density = cv2.resize(density, self.img_size, interpolation=cv2.INTER_LINEAR)\n",
    "                density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "                return img, density\n",
    "        elif os.path.exists(mat_path):\n",
    "            try:\n",
    "                mat = loadmat(mat_path)\n",
    "                points = mat['image_info'][0,0]['location'][0,0]\n",
    "            except NotImplementedError:\n",
    "                with h5py.File(mat_path, 'r') as f:\n",
    "                    points = np.array(f['image_info'][0,0][0,0][0])\n",
    "            if points.size == 0:\n",
    "                coords = np.empty((0, 2), dtype=np.int32)\n",
    "            else:\n",
    "                coords = np.array(points)\n",
    "            img = img.resize(self.img_size)\n",
    "            h, w = img.size[1], img.size[0]\n",
    "            k = np.zeros((h, w))\n",
    "            for i in range(coords.shape[0]):\n",
    "                x = min(int(coords[i][0]), w-1)\n",
    "                y = min(int(coords[i][1]), h-1)\n",
    "                if y < h and x < w:\n",
    "                    k[y, x] = 1\n",
    "            density = gaussian_filter_density(k)\n",
    "            scale_factor = (self.img_size[0] * self.img_size[1]) / (h * w)\n",
    "            density = cv2.resize(density, self.img_size, interpolation=cv2.INTER_LINEAR)\n",
    "            density = density * scale_factor\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "            return img, density\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Ground truth file not found: {mat_path} or {h5_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# CSRNet Model\n",
    "# -------------------------------\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSRNet, self).__init__()\n",
    "        vgg = models.vgg16_bn(weights=models.VGG16_BN_Weights.DEFAULT)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:33]) \n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 256, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.output_layer = nn.Conv2d(64, 1, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "def train_model(num_epochs=5, batch_size=4, learning_rate=1e-5, save_every=5):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    train_dataset = CrowdDataset(TRAIN_IMG_DIR, TRAIN_GT_DIR, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        for imgs, densities in train_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            target_densities_downsampled = F.interpolate(\n",
    "                densities, \n",
    "                size=(OUTPUT_SIZE, OUTPUT_SIZE), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            target_densities_downsampled = target_densities_downsampled * (DOWNSAMPLE_FACTOR ** 2)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, target_densities_downsampled) \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), \"csrnet_partA_best.pth\")\n",
    "            print(f\"Best model saved with loss: {best_loss:.6f}\")\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            torch.save(model.state_dict(), f\"csrnet_partA_epoch_{epoch+1}.pth\")\n",
    "    torch.save(model.state_dict(), \"csrnet_partA_final.pth\")\n",
    "    print(\"Final model saved to csrnet_partA_final.pth\")\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------\n",
    "def evaluate_model(model):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_dataset = CrowdDataset(TEST_IMG_DIR, TEST_GT_DIR, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    print(\"Evaluating model...\")\n",
    "    mae, rmse, n = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, densities in val_loader:\n",
    "            imgs, densities = imgs.to(DEVICE), densities.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            predicted_count = max(outputs.sum().item(), 0)\n",
    "            actual_count = densities.sum().item()\n",
    "            mae += abs(predicted_count - actual_count)\n",
    "            rmse += (predicted_count - actual_count) ** 2\n",
    "            n += 1\n",
    "    mae /= n\n",
    "    rmse = np.sqrt(rmse / n)\n",
    "    print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "    return mae, rmse\n",
    "\n",
    "# -------------------------------\n",
    "# Alert System\n",
    "# -------------------------------\n",
    "def alert_system(model, img_path, crowd_limit=50):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_resized = img.resize((IMG_WIDTH, IMG_HEIGHT))\n",
    "    img_t = transform(img_resized).unsqueeze(0).to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_t)\n",
    "    count = max(int(output.sum().item()), 0)\n",
    "    if count > crowd_limit:\n",
    "        print(f\"🚨 ALERT: Overcrowded! Count: {count} (Limit: {crowd_limit})\")\n",
    "        return True, count\n",
    "    else:\n",
    "        print(f\"✅ Normal crowd level. Count: {count} (Limit: {crowd_limit})\")\n",
    "        return False, count\n",
    "\n",
    "def process_images_with_alerts(model, img_dir, crowd_limit=50):\n",
    "    image_files = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "    alert_count = 0\n",
    "    total_images = len(image_files)\n",
    "    if total_images == 0:\n",
    "        print(f\"No images found in {img_dir}\")\n",
    "        return\n",
    "    for img_path in image_files:\n",
    "        filename = os.path.basename(img_path)\n",
    "        is_alert, count = alert_system(model, img_path, crowd_limit)\n",
    "        status = \"🚨 ALERT\" if is_alert else \"✅ NORMAL\"\n",
    "        print(f\"{filename:<20} | Count: {count:>4} | {status}\")\n",
    "        if is_alert:\n",
    "            alert_count += 1\n",
    "    print(f\"Summary: {alert_count}/{total_images} images triggered alerts\")\n",
    "    print(f\"Alert percentage: {(alert_count/total_images)*100:.1f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Model\n",
    "# -------------------------------\n",
    "def load_model(model_path=\"csrnet_partA_best.pth\"):\n",
    "    model = CSRNet().to(DEVICE)\n",
    "    if os.path.exists(model_path):\n",
    "        state_dict = torch.load(model_path, map_location=DEVICE)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        print(f\"Loaded model from {model_path} (strict=False)\")\n",
    "        return model, True\n",
    "    return model, False\n",
    "\n",
    "# -------------------------------\n",
    "# Main\n",
    "# -------------------------------\n",
    "def main():\n",
    "    model, model_loaded = load_model()\n",
    "    if not model_loaded:\n",
    "        print(\"Training new model...\")\n",
    "        model = train_model(num_epochs=5, batch_size=4, learning_rate=1e-5)\n",
    "    evaluate_model(model)\n",
    "    if os.path.exists(TEST_IMG_DIR):\n",
    "        process_images_with_alerts(model, TEST_IMG_DIR, crowd_limit=30)\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd329c-8e8c-4a35-a3af-338310fd549c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
